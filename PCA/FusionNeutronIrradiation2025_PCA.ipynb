{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86eccc2d-f2ff-4380-b834-6f74158aeb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "__nbid__ = '0001'\n",
    "__author__ = 'Felix Pat <felixpat@berkeley.edu>'\n",
    "__version__ = '20251014' # yyyymmdd\n",
    "__datasets__ = ['Pristine', 'SCSHD', 'SCSLD', 'SCS88in']\n",
    "__keywords__ = ['Angular Dispersion', 'Principal Component Analysis', 'Contrast Limited Adaptive Histogram Equalization', 'Fast Fourier Transform']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73778086-2da0-4171-adb1-811211092d0d",
   "metadata": {
    "tags": []
   },
   "source": [
    "#  Eﬀects of Fission versus Fusion-like Neutron Spectra on REBCO Activation and Microstructure: TEM Analysis\n",
    "*Author(s): Felix Pat (Univ. of California, Berkeley & Urenco USA)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152954b6-1928-415c-8626-ac2bf8e4f0ea",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "* [Goal](#Ngoal)\n",
    "* [Summary](#Nsummary)\n",
    "* [Disclaimer & Attribution](#Ndisclaimer)\n",
    "* [Imports & Setup](#Nimport)\n",
    "* [Read and Preprocess TEM Images](#N0)\n",
    "* [PCA Analysis for Angular Dispersion Distribution](#N1)\n",
    "* [Filter Angular Dispersion Distribution](#N2)\n",
    "* [Plot Angular Dispersion versus Irradiation Dose Boxplots](#N3)\n",
    "* [References](#N4)\n",
    "* [Appendix](#N5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96d0645-fe9f-4559-b01f-fbe44faf8417",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<a class=\"anchor\" id=\"Ngoal\"></a>\n",
    "#  Goal\n",
    "This notebook aims to demonstrate a step-by-step workflow from preprocessing TEM images to quantifying the angular dispersion with respect to irradiation dose."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63698e5f-7043-4148-be81-805932edf5df",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"Nsummary\"></a>\n",
    "#  Summary\n",
    "This notebook demonstrates how to generally:\n",
    "1. Read TEM image files in the forms of real space (.dm3), FFT (.dm5), and masked array inverse FFTs TIF images. Default image sizes are 1024x1024, 512x512, and 384x384\n",
    "2. Standardize and save all PNG images to 6000x6000 while applying an image equalization method, Contrast Limited Adaptive Histogram Equalization, to increase the brightness.\n",
    "3. \n",
    "**For** each **image** in directory:\n",
    "    - Read image_path.png in binary gray scale \n",
    "    - Apply Otsu's thresholding \n",
    "    - Segment contours and apply Border Following Algorithm \\cite{} via Teh-Chin chain approximation algorithm with a full family hierarchy list tree \n",
    "    - **For** each **contour** in image above minimum size (hyperparameter) \n",
    "        - Apply Principal Component Analysis for the primary component's eigenvector and eigenvalue \n",
    "        - Calculate the angular dispersion $\\theta$ of PCA eigenvector via 2-argument arctangent $\\theta$ $\\epsilon$ [-$\\pi$, $\\pi$] \n",
    "    - **End For**\n",
    "    - Modulo $\\pi$ angular dispersion $\\theta$ to model anisotropic behavior such that $\\theta$ $\\epsilon$ [0, $\\pi$] \n",
    "    - Filter the bottom 40th percentile of angular dispersion $\\theta$ distribution for pristine images only \n",
    "    - Filter angular dispersion $\\theta$ distribution by the 70th percentile to 97th percentile contours by area, which correlates with the dominant eigenvectors sorted by large PCA eigenvalues \n",
    "    - Apply a multi-gaussian fit (try curve_fit else norm.fit) on each images' angular dispersion distribution  \n",
    "\n",
    "**End For** <br>\n",
    "4. Plot boxplots of each datasets' angular dispersion versus irradiation dose\n",
    "\n",
    "**Note:** Order of pathways to image in directory differs depending on how it is read! This will change your labeling. Match output.txt file to your glob(image_directory) to ensure your angles.npy order matches the correct image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb45f4d-4c90-4b2b-9c06-6b7e2ede9135",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"Ndisclaimer\"></a>\n",
    "#  Disclaimer & Attribution\n",
    "\n",
    "Disclaimers\n",
    "-----------\n",
    "Note that using the following work constitutes your agreement with our minimal [Disclaimers]().\n",
    "\n",
    "If you use this work in your published research, **please cite** the following paper(s):\n",
    "\n",
    "Acknowledgments\n",
    "---------------\n",
    "The work done at the University of California, Berkley, was supported by the U.S. Department of Energy\n",
    "(DOE) through the Oﬃce of Fusion Energy Sciences under contract to the University of California, Santa\n",
    "Barbra DOE-Fusion (No. DE-FG02-94ER54275). The work at the UC Berkeley’s College of Engineering was\n",
    "supported by the Fung Institute for Engineering Leadership’s Master of Engineering degree program,\n",
    "whose capstone component provided the student team that spearheaded this endeavor. Here, the\n",
    "authors would like to thank [put the other teammates]. The authors would also like to thank Andrew\n",
    "Gubser and Jeﬀrey Bickel for technical support. The previous work done by our colleagues in Japan was\n",
    "performed under the GIMRT Program of the Institute for Materials Research, Tohoku University\n",
    "(Proposal Nos. 202012-IRKMA-0051 and 202212-IRKMA-0507). The previous work was supported in part\n",
    "by JSPS KAKENHI Grant Nos. JP16H06008, JP18KK0087, JP23H03665, and JP23K28354. The work at LBNL,\n",
    "including 88-Inch Cyclotron, Berkeley Center for Magnet Technology, and the National Center for\n",
    "Electron Microscopy at the Molecular Foundry, was supported by the Director, Oﬃce of Sciences, Oﬃce\n",
    "of High Energy Physics, Oﬃce of Fusion Energy Sciences, and Oﬃce of Basic Energy Sciences, of the U.S.\n",
    "Department of Energy under Contract No. DE-AC02-05CH11231 and through a US–Japan High Energy\n",
    "Physics Collaboration grant. The experimental work at the 88-Inch Cyclotron was additionally supported\n",
    "by the U.S. Nuclear Data Program in the DOE Oﬃce of Nuclear Physics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c9e19f-d33b-4d47-996d-1df3ae94a0e8",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"Nimport\"></a>\n",
    "#  Imports and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc45183b-2831-4c41-8597-95353d22296d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda env\n",
    "# ipython == 8.15.0\n",
    "# python == 3.9.18\n",
    "\n",
    "# std lib\n",
    "import os\n",
    "import copy\n",
    "from math import atan2, cos, sin, sqrt, pi\n",
    "import glob\n",
    "\n",
    "# 3rd party with versions listed\n",
    "import numpy as np                              # 1.24.0\n",
    "from scipy.stats import norm                    # 1.13.1\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.signal import find_peaks\n",
    "import torch                                    # 2.1.0\n",
    "import cv2                                      # 3.4.18.65\n",
    "import ncempy                                   # 1.11.1\n",
    "import h5py                                     # 3.10.0\n",
    "import tifffile                                 # 2024.2.12\n",
    "\n",
    "# visualization purposes\n",
    "from typing import List\n",
    "import numpy.typing as npt\n",
    "import matplotlib as mpl                        # 3.9.4\n",
    "from matplotlib import pyplot as plt\n",
    "from bokeh.plotting import ColumnDataSource, figure, output_file, show, output_notebook\n",
    "\n",
    "mpl.rcParams['figure.dpi'] = 500 # 200 to read in notebook, 300 for final saving of plots\n",
    "%matplotlib inline\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886dda0d",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"N0\"></a>\n",
    "#  Read Directory Paths with iFFT TEM TIF Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c44721d-2442-44f5-ad40-d31fb9776c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_ARRAY_PRISTINE ='/global/cfs/cdirs/m4361/2025/Array/prist/*.tif'\n",
    "DIR_ARRAY_HD ='/global/cfs/cdirs/m4361/2025/Array/HD/*.tif'\n",
    "DIR_ARRAY_LD ='/global/cfs/cdirs/m4361/2025/Array/LD/*.tif'\n",
    "DIR_ARRAY_88 ='/global/cfs/cdirs/m4361/2025/Array/88in/*.tif'\n",
    "\n",
    "FILE_TYPES = ['.dm3', '.dm5', '.tif', '.png', '.jpg']\n",
    "\n",
    "def read_file_paths_in_directory(image_directory: str, preprocessed: bool = False) -> List[str]:\n",
    "    \"\"\"\n",
    "    Retrieve the (image) file pathways given a directory\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    image_directory : str\n",
    "    Path to directory with image files\n",
    "\n",
    "    preprocessed : bool\n",
    "    Indicator for what files to read\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    paths : List[str]\n",
    "    List of paths to images found in image directory\n",
    "    \"\"\"\n",
    "    \n",
    "    paths = []\n",
    "\n",
    "    for path in glob.glob(image_directory):\n",
    "        if preprocessed and 'pca' not in path and 'contrast' in path:\n",
    "            paths.append(path)\n",
    "        elif not preprocessed and 'pca' not in path and 'contrast' not in path: # personal identifiers to exclude repeats\n",
    "            paths.append(path)\n",
    "            \n",
    "    return paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357cdff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATHS_PRISTINE = read_file_paths_in_directory(DIR_ARRAY_PRISTINE)\n",
    "PATHS_HD = read_file_paths_in_directory(DIR_ARRAY_HD)\n",
    "PATHS_LD = read_file_paths_in_directory(DIR_ARRAY_LD)\n",
    "PATHS_88 = read_file_paths_in_directory(DIR_ARRAY_88)\n",
    "print(len(PATHS_PRISTINE), len(PATHS_HD), len(PATHS_LD), len(PATHS_88))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e3fa05",
   "metadata": {},
   "source": [
    "<a class=\"anchor\"></a>\n",
    "#  Preprocess iFFT TEM TIF Images and Save as PNG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0439b999",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_upscale_image(image_paths: List[str], file_type: str) -> None:\n",
    "    \"\"\"\n",
    "    Given list of pathways, preprocess iFFT TIF images using CLAHE and save at fixed resolution 6000x6000\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    image_paths : List[str]\n",
    "    Paths to image files\n",
    "\n",
    "    file_type : str\n",
    "    Type of image file in paths to image files\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    \n",
    "    plt.ioff()\n",
    "    \n",
    "    for i, path in enumerate(image_paths):\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(12,12))\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        fig.subplots_adjust(0,0,1,1)\n",
    "        \n",
    "\n",
    "        clahe = cv2.createCLAHE(clipLimit=8, tileGridSize=(16,16)) # hyperparameters for CLAHE\n",
    "        ax.imshow(clahe.apply(cv2.normalize(tifffile.imread(path), None, 0, 255, cv2.NORM_MINMAX).astype('uint8'))) # read TIF image in binary\n",
    "\n",
    "        fig.savefig(path.replace(file_type, '_contrast.png'), bbox_inches='tight', pad_inches = 0)\n",
    "    \n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ffb44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_upscale_image(PATHS_PRISTINE, FILE_TYPES[2])\n",
    "save_upscale_image(PATHS_HD, FILE_TYPES[2])\n",
    "save_upscale_image(PATHS_LD, FILE_TYPES[2])\n",
    "save_upscale_image(PATHS_88, FILE_TYPES[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42761c11",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"N1\"></a>\n",
    "#  PCA Analysis for Angular Dispersion Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79faf6d6-c828-4982-98d8-89020aa93095",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_orientation(points: npt.NDArray[float], img: npt.NDArray[float]) -> float:\n",
    "    \"\"\"\n",
    "    Given points of a contour, calculate the PCA eigenvector and eigenvalue and \n",
    "    use atan2(eigenvector) to calculate orientation of contour i.e. our dispersion angle\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    points : npt.NDArray[float]\n",
    "    Points of contour from cv2.findContours\n",
    "    \n",
    "    img : npt.NDArray[float]\n",
    "    Input TEM image\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    angle : float\n",
    "    Dispersion angle in radians from atan2\n",
    "    \"\"\"\n",
    "\n",
    "    contour_size = len(points)\n",
    "    data_points = np.empty((contour_size, 2), dtype=np.float64)\n",
    "    for i in range(data_points.shape[0]):\n",
    "        data_points[i,0] = points[i,0,0]\n",
    "        data_points[i,1] = points[i,0,1]\n",
    "\n",
    "    #  Perform PCA analysis\n",
    "    mean = np.empty((0))\n",
    "    mean, eigenvectors, eigenvalues = cv2.PCACompute2(data_points, mean)\n",
    "\n",
    "  \n",
    "    center = (int(mean[0,0]), int(mean[0,1]))\n",
    "    cv2.circle(img, center, 5, (255, 0, 255), 2) # draw a circle on center of contour\n",
    "   \n",
    "    scaled_eigenvector = (center[0] + 0.02 * eigenvectors[0,0] * eigenvalues[0,0], center[1] + 0.02 *  eigenvectors[0,1] * eigenvalues[0,0])\n",
    "    draw_axis(img, center, scaled_eigenvector, (255, 0 , 0), 1) # draw eigenvector of contour\n",
    "\n",
    "    angle = atan2(eigenvectors[0,1], eigenvectors[0,0]) #  orientation in radians\n",
    "\n",
    "    return angle\n",
    "    \n",
    "def draw_axis(img: npt.NDArray[float], center_: tuple[float, float], eigenvector_: tuple[float, float], colour: tuple[float, float, float], scale: int) -> None:\n",
    "    \"\"\"\n",
    "    Given eigenvector of contour, draw the eigenvector on input TEM image\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    img : npt.NDArray[float]\n",
    "    Input TEM image\n",
    "\n",
    "    center_ : tuple[float, float]\n",
    "    Center coordinate of contour\n",
    "\n",
    "    eigenvector_ : tuple[float, float]\n",
    "    Eigenvector coordinate of contour\n",
    "\n",
    "    colour : tuple[float, float, float]\n",
    "    Set eigenvector draw color parameter\n",
    "    \n",
    "    scale : int\n",
    "    Size of eigenvector to draw\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    \n",
    "    center = list(center_)\n",
    "    eigenvector = list(eigenvector_)\n",
    "\n",
    "    angle = atan2(center[1] - eigenvector[1], center[0] - eigenvector[0]) #  angle in radians\n",
    "    hypotenuse = sqrt((center[1] - eigenvector[1]) * (center[1] - eigenvector[1]) + (center[0] - eigenvector[0]) * (center[0] - eigenvector[0]))\n",
    "\n",
    "    #  Scale and draw arrow\n",
    "    eigenvector[0] = center[0] - scale * hypotenuse * cos(angle)\n",
    "    eigenvector[1] = center[1] - scale * hypotenuse * sin(angle)\n",
    "    cv2.line(img, (int(center[0]), int(center[1])), (int(eigenvector[0]), int(eigenvector[1])), colour, 10, cv2.LINE_AA)\n",
    "\n",
    "    #  Draw arrow hooks\n",
    "    center[0] = eigenvector[0] + 9 * cos(angle + pi / 4)\n",
    "    center[1] = eigenvector[1] + 9 * sin(angle + pi / 4)\n",
    "    cv2.line(img, (int(center[0]), int(center[1])), (int(eigenvector[0]), int(eigenvector[1])), colour, 10, cv2.LINE_AA)\n",
    "    center[0] = eigenvector[0] + 9 * cos(angle - pi / 4)\n",
    "    center[1] = eigenvector[1] + 9 * sin(angle - pi / 4)\n",
    "    cv2.line(img, (int(center[0]), int(center[1])), (int(eigenvector[0]), int(eigenvector[1])), colour, 10, cv2.LINE_AA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d21eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA_analysis(directory: List[str], minimum_area: float = 1e3) -> None:\n",
    "    \"\"\"\n",
    "    Main function that performs PCA analysis given directory list of preprocessed TEM images\n",
    "    and saves .npy angular dispersion distribution and contour areas of each image with\n",
    "    annotated grayscale image of eigenvector and contours\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    directory : List[str]\n",
    "    Path of each class of TEM image directory\n",
    "\n",
    "\n",
    "    minimum_area : float\n",
    "    Filter for minimum contour area default to 1000 pixels\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    store_angle, store_area, paths = [], [], []\n",
    "\n",
    "    image_count = 0\n",
    "    \n",
    "    for dir in directory:\n",
    "        for path in glob.glob(dir):\n",
    "\n",
    "            if 'contrast' in path and 'pca' not in path: # select preprocessed PNG images\n",
    "                paths.append(path)\n",
    "                image_count+=1\n",
    "       \n",
    "        with open(\"PCA_output.txt\", \"a\") as f:\n",
    "            print(len(paths), file=f)\n",
    "        print(len(paths)) # print number of image paths read from each directory\n",
    "    \n",
    "    # print total number of TEM image paths\n",
    "    with open(\"PCA_output.txt\", \"a\") as f:\n",
    "        print(len(paths), file=f)\n",
    "    print(len(paths))\n",
    "    for i, path in enumerate(paths):\n",
    "        \n",
    "        image_source = cv2.imread(path)\n",
    "\n",
    "        #  convert to grayscale\n",
    "        image_gray = cv2.cvtColor(image_source, cv2.COLOR_BGR2GRAY)\n",
    "        # final image used to draw and save\n",
    "        image_final = cv2.cvtColor(image_gray, cv2.COLOR_GRAY2BGR)\n",
    "    \n",
    "        #  convert img into binary with Otsu's thresholding\n",
    "        _, image_threshold = cv2.threshold(image_gray, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n",
    "        #  calculate points of contours\n",
    "        _, contours, _ = cv2.findContours(image_threshold, cv2.RETR_TREE, cv2.CHAIN_APPROX_TC89_L1) # methods = [CHAIN_APPROX_NONE, CHAIN_APPROX_SIMPLE, cv2.CHAIN_APPROX_TC89_L1, cv2.CHAIN_APPROX_TC89_KCOS]\n",
    "    \n",
    "        image_angles = []\n",
    "        contour_areas = []\n",
    "        \n",
    "        for j,c in enumerate(contours):\n",
    "          #  area of each contour\n",
    "          area = cv2.contourArea(c)\n",
    "            \n",
    "          if area < minimum_area:\n",
    "            continue\n",
    "\n",
    "          #  draw each contour on grayscale image\n",
    "          cv2.drawContours(image_final, contours, j, (0, 0, 255), 2)\n",
    "\n",
    "          #  find orientation of each shape\n",
    "          image_angles.append(get_orientation(c,image_final))\n",
    "          contour_areas.append(area)\n",
    "\n",
    "        store_angle = store_angle + [image_angles]\n",
    "        store_area = store_area + [contour_areas]\n",
    "    \n",
    "        with open(\"PCA_output.txt\", \"a\") as f:\n",
    "            print(len(image_angles), len(contour_areas), path, file=f)\n",
    "            \n",
    "        fig,ax=plt.subplots(figsize=(20,20))\n",
    "        ax.imshow(image_final)\n",
    "        ax.axis('off')\n",
    "        fig.savefig(path.replace(FILE_TYPES[3],'') + '_pca_final_%i.png'%i, bbox_inches = 'tight', pad_inches = 0)\n",
    "        plt.close()\n",
    "\n",
    "    #  indices separating classes stored in output txt file\n",
    "    a = np.array(store_angle, dtype=object)\n",
    "    np.save('pca_final_angle%d.npy'%len(paths), a)\n",
    "    b = np.array(store_area, dtype=object)\n",
    "    np.save('pca_final_area%d.npy'%len(paths), b)\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af7b849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Reading in PNG will result in different order pathways compared to TIF, which is important to know for saved angles.npy order\n",
    "PCA_DIRECTORIES = ['/global/cfs/cdirs/m4361/2025/Array/prist/*.png','/global/cfs/cdirs/m4361/2025/Array/HD/*.png','/global/cfs/cdirs/m4361/2025/Array/LD/*.png','/global/cfs/cdirs/m4361/2025/Array/88in/*.png']\n",
    "    \n",
    "PCA_analysis(PCA_DIRECTORIES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79edc416",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"N2\"></a>\n",
    "#  Filter Angular Dispersion Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e84600-3591-40e8-87d1-b8f282d7a71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update order of pathways to match order of angle array\n",
    "DIR_ARRAY_PRISTINE ='/global/cfs/cdirs/m4361/2025/Array/prist/*.png'\n",
    "DIR_ARRAY_HD ='/global/cfs/cdirs/m4361/2025/Array/HD/*.png'\n",
    "DIR_ARRAY_LD ='/global/cfs/cdirs/m4361/2025/Array/LD/*.png'\n",
    "DIR_ARRAY_88 ='/global/cfs/cdirs/m4361/2025/Array/88in/*.png'\n",
    "\n",
    "PATHS_PRISTINE = read_file_paths_in_directory(DIR_ARRAY_PRISTINE, preprocessed=True)\n",
    "PATHS_HD = read_file_paths_in_directory(DIR_ARRAY_HD, preprocessed=True)\n",
    "PATHS_LD = read_file_paths_in_directory(DIR_ARRAY_LD, preprocessed=True)\n",
    "PATHS_88 = read_file_paths_in_directory(DIR_ARRAY_88, preprocessed=True)\n",
    "print(len(PATHS_PRISTINE), len(PATHS_HD), len(PATHS_LD), len(PATHS_88))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5d0d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRISTINE_IDX = len(PATHS_PRISTINE)\n",
    "HD_IDX = PRISTINE_IDX + len(PATHS_HD)\n",
    "LD_IDX = HD_IDX + len(PATHS_LD)\n",
    "\n",
    "\n",
    "ANGLES_PRISTINE = np.load('/global/cfs/cdirs/m4361/2025/analysis/pcafinal_angle416.npy',allow_pickle=True)[:PRISTINE_IDX]\n",
    "ANGLES_HD = np.load('/global/cfs/cdirs/m4361/2025/analysis/pcafinal_angle416.npy',allow_pickle=True)[PRISTINE_IDX:HD_IDX]\n",
    "ANGLES_LD = np.load('/global/cfs/cdirs/m4361/2025/analysis/pcafinal_angle416.npy',allow_pickle=True)[HD_IDX:LD_IDX]\n",
    "ANGLES_88 = np.load('/global/cfs/cdirs/m4361/2025/analysis/pcafinal_angle416.npy',allow_pickle=True)[LD_IDX:]\n",
    "\n",
    "WEIGHTS_PRISTINE = np.load('/global/cfs/cdirs/m4361/2025/analysis/pcafinal_area416.npy',allow_pickle=True)[:PRISTINE_IDX]\n",
    "WEIGHTS_HD = np.load('/global/cfs/cdirs/m4361/2025/analysis/pcafinal_area416.npy',allow_pickle=True)[PRISTINE_IDX:HD_IDX]\n",
    "WEIGHTS_LD = np.load('/global/cfs/cdirs/m4361/2025/analysis/pcafinal_area416.npy',allow_pickle=True)[HD_IDX:LD_IDX]\n",
    "WEIGHTS_88 = np.load('/global/cfs/cdirs/m4361/2025/analysis/pcafinal_area416.npy',allow_pickle=True)[LD_IDX:]\n",
    "\n",
    "len(ANGLES_PRISTINE),len(ANGLES_HD),len(ANGLES_LD),len(ANGLES_88)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14b91d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_filter(angles: npt.NDArray[object], paths: List[str]) -> dict[str, List[float]]:\n",
    "    \"\"\"\n",
    "    Ensure order of angles and paths correspond to the correct image, filter angular\n",
    "    dispersion distributions by labels: magnification and where TEM image was taken\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    angles : npt.NDArray[object]\n",
    "    Angular disperesion distribution of each image\n",
    "\n",
    "    paths : List[str]\n",
    "    List of paths to images found in image directory\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    angle_dict : dict[str, List[float]]\n",
    "    Angular dispersion distributions labeled by magnification and where TEM was taken\n",
    "    \"\"\"\n",
    "\n",
    "    mag = ['26k', '38k', '43k', '66k', '92k']\n",
    "    where = ['edge', 'BZO']\n",
    "\n",
    "    angle_dict = {\n",
    "        'edge_26k': [],\n",
    "        'edge_38k': [],\n",
    "        'edge_43k': [],\n",
    "        'edge_66k': [],\n",
    "        'edge_92k': [],\n",
    "        'rs_26k': [],\n",
    "        'rs_38k': [],\n",
    "        'rs_43k': [],\n",
    "        'rs_66k': [],\n",
    "        'rs_92k': [],\n",
    "        'bzo_26k': [],\n",
    "        'bzo_38k': [],\n",
    "        'bzo_43k': [],\n",
    "        'bzo_66k': [],\n",
    "        'bzo_92k': [],\n",
    "    }\n",
    "\n",
    "    for i in range(len(angles)):\n",
    "        if mag[0] in paths[i] and where[0] in paths[i]:\n",
    "            angle_dict['edge_26k'].append(i)\n",
    "        elif mag[1] in paths[i] and where[0] in paths[i]:\n",
    "            angle_dict['edge_38k'].append(i)\n",
    "        elif mag[2] in paths[i] and where[0] in paths[i]:\n",
    "            angle_dict['edge_43k'].append(i)\n",
    "        elif mag[3] in paths[i] and where[0] in paths[i]:\n",
    "            angle_dict['edge_66k'].append(i)\n",
    "        elif mag[4] in paths[i] and where[0] in paths[i]:\n",
    "            angle_dict['edge_92k'].append(i)\n",
    "\n",
    "        elif mag[0] in paths[i] and where[1] in paths[i]:\n",
    "            angle_dict['bzo_26k'].append(i)\n",
    "        elif mag[1] in paths[i] and where[1] in paths[i]:\n",
    "            angle_dict['bzo_38k'].append(i)\n",
    "        elif mag[2] in paths[i] and where[1] in paths[i]:\n",
    "            angle_dict['bzo_43k'].append(i)\n",
    "        elif mag[3] in paths[i] and where[1] in paths[i]:\n",
    "            angle_dict['bzo_66k'].append(i)\n",
    "        elif mag[4] in paths[i] and where[1] in paths[i]:\n",
    "            angle_dict['bzo_92k'].append(i)\n",
    "            \n",
    "        elif mag[0] in paths[i]:\n",
    "            angle_dict['rs_26k'].append(i)\n",
    "        elif mag[1] in paths[i]:\n",
    "            angle_dict['rs_38k'].append(i)\n",
    "        elif mag[2] in paths[i]:\n",
    "            angle_dict['rs_43k'].append(i)\n",
    "        elif mag[3] in paths[i]:\n",
    "            angle_dict['rs_66k'].append(i)\n",
    "        elif mag[4] in paths[i]:\n",
    "            angle_dict['rs_92k'].append(i)\n",
    "   \n",
    "    for i, val in enumerate(angle_dict.values()):\n",
    "        \n",
    "        if i % 5 == 0:\n",
    "            print()\n",
    "        print(len(val), end=' ')\n",
    "        \n",
    "    print()\n",
    "        \n",
    "    return angle_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1564c98f-4c84-43ec-b35f-7dbf2e7224fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANGLES_PRISTINE_LABELED = label_filter(ANGLES_PRISTINE, PATHS_PRISTINE)\n",
    "ANGLES_HD_LABELED = label_filter(ANGLES_HD, PATHS_HD)\n",
    "ANGLES_LD_LABELED = label_filter(ANGLES_LD, PATHS_LD)\n",
    "ANGLES_88_LABELED = label_filter(ANGLES_88, PATHS_88)\n",
    "\n",
    "\"\"\" \n",
    "print output in (row, column) format\n",
    "            26k 38k 43k 66k 92k\n",
    "Edge\n",
    "Real Space\n",
    "BZO\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def92c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modulo_pi(angles: npt.NDArray[object]) -> npt.NDArray[object]:\n",
    "    \"\"\"\n",
    "    Modulo pi angular dispersion disrtribution such that angles range 0 to pi\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    angles : npt.NDArray[object]\n",
    "    Angular dispersion distribution of each image\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    angles : npt.NDArray[object]\n",
    "    Angular dispersion distribution of each image\n",
    "    \"\"\"\n",
    "    \n",
    "    for i in range(len(angles)):\n",
    "        angles[i] = [(k + np.pi if k <= 0. else k) for k in angles[i]]\n",
    "\n",
    "    return angles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3616712c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_filter(angles: npt.NDArray[object], weights: npt.NDArray[object], upper_percentile: float = 97, pristine: bool = False) -> npt.NDArray[object]:\n",
    "    \"\"\"\n",
    "    Filter angular dispersion distribution by contour size i.e. eigenvalue\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    angles: npt.NDArray[object]\n",
    "    Angular dispersion distribution of each image\n",
    "\n",
    "    weights: npt.NDArray[object]\n",
    "    Contour areas of each angle of each image\n",
    "\n",
    "    upper_percentile: float = 97\n",
    "    Upper percentile filter on contour areas\n",
    "\n",
    "    pristine : bool\n",
    "    Input data is pristine image data or not\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    angles_filtered : npt.NDArray(object)\n",
    "    Angular dispersion distribution after contour size filters\n",
    "    \"\"\"\n",
    "   \n",
    "    angles_filtered = angles.copy()\n",
    "\n",
    "    for i, angle in enumerate(angles):\n",
    "        \n",
    "        threshold1 = np.percentile(weights[i], upper_percentile)\n",
    "        threshold2 = np.percentile(weights[i], 70)\n",
    "        \n",
    "        if pristine:\n",
    "            threshold3 = np.percentile(angle, 40)\n",
    "            mask = (weights[i] >= threshold2) & (weights[i] <= threshold1) & (angle > threshold3)\n",
    "            \n",
    "        else:\n",
    "            mask = (weights[i] >= threshold2) & (weights[i] <= threshold1)\n",
    "\n",
    "        angles_filtered[i] = np.array(angle)[mask]\n",
    "  \n",
    "    return angles_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9251b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANGLES_PRISTINE = modulo_pi(ANGLES_PRISTINE)\n",
    "ANGLES_HD = modulo_pi(ANGLES_HD)\n",
    "ANGLES_LD = modulo_pi(ANGLES_LD)\n",
    "ANGLES_88 = modulo_pi(ANGLES_88)\n",
    "\n",
    "ANGLES_PRISTINE_FILTERED = weight_filter(ANGLES_PRISTINE, WEIGHTS_PRISTINE, pristine = True)\n",
    "ANGLES_HD_FILTERED = weight_filter(ANGLES_HD, WEIGHTS_HD)\n",
    "ANGLES_LD_FILTERED = weight_filter(ANGLES_LD, WEIGHTS_LD)\n",
    "ANGLES_88_FILTERED = weight_filter(ANGLES_88, WEIGHTS_88)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d10d44",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"N3\"></a>\n",
    "# Plot Boxplots Angular Dispersion versus Irradiation Dose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c8c5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_gaussian(x: float, *params: float) -> float:\n",
    "    \"\"\"\n",
    "    Gaussian distribution function for curve_fit\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : float\n",
    "    Data point at x\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    y : float\n",
    "    Data point estimated at y\n",
    "    \"\"\"\n",
    "    \n",
    "    y = np.zeros_like(x)\n",
    "\n",
    "    for i in range(len(params) // 3):\n",
    "        amp, mean, std = params[3*i:3*i+3]\n",
    "        y += amp * np.exp(-((x - mean)**2) / (2 * std**2))\n",
    "\n",
    "    return y\n",
    "\n",
    "def guess_initial_params_uniform(x: List[float], y: List[float], n_components: int) -> List[float]:\n",
    "    \"\"\"\n",
    "    Setup curve_fit initial guess for multigaussian distribution\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : float\n",
    "    Data point at x\n",
    "\n",
    "    y : float\n",
    "    Data point at y\n",
    "\n",
    "    n_components : int\n",
    "    N number of initial guesses at different means which will output N number of gaussians\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    init_params : List[float]\n",
    "    Gaussian distribution initial guess of parameters\n",
    "    \"\"\"\n",
    "\n",
    "    amp_guess = max(y) / n_components\n",
    "    mean_guess = np.linspace(min(x), max(x), n_components)\n",
    "    if n_components == 1 and min(x) < 3/4 * np.pi and max(x) > 3/4 * np.pi:\n",
    "        mean_guess = np.array([3/4 * np.pi])\n",
    "    std_guess = [(max(x) - min(x)) / (2 * n_components)] * n_components\n",
    "    init_params = []\n",
    "\n",
    "    for a, m, s in zip([amp_guess]*n_components, mean_guess, std_guess):\n",
    "        init_params.extend([a, m, s])\n",
    "\n",
    "    return init_params\n",
    "\n",
    "def fit_multi_gaussian(x: List[float], y: List[float], n_components: int = 3) -> List[float]:\n",
    "    \"\"\"\n",
    "    Setup curve_fit initial guess for multigaussian distribution\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : float\n",
    "    Data point at x\n",
    "\n",
    "    y : float\n",
    "    Data point at y\n",
    "\n",
    "    n_components : int\n",
    "    N number of initial guesses at different means which will output N number of gaussians\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    init_params : List[float]\n",
    "    Gaussian distribution initial guess of parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    lower_bounds = []\n",
    "    upper_bounds = []\n",
    "    \n",
    "    for i in range(n_components):\n",
    "        lower_bounds.extend([0, min(x), 0.01])  #  amp ≥ 0, mean ≥ min(x), std ≥ 0.01\n",
    "        upper_bounds.extend([max(y)*2, max(x), np.pi/2])  #  std can span full range\n",
    "\n",
    "    init_params = guess_initial_params_uniform(x, y, n_components)\n",
    "    fit_params, _ = curve_fit(multi_gaussian, x, y, p0=init_params, bounds=(lower_bounds, upper_bounds))\n",
    "\n",
    "    return fit_params\n",
    "\n",
    "def plot_hists(image_angles: npt.NDArray[object], components : int = 3) -> tuple[List[float], List[float]]:\n",
    "    \"\"\"\n",
    "    Retrieve the \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    image_angles : npt.NDArray[object]\n",
    "    Angular dispersion distribution of each image\n",
    "\n",
    "    components : int\n",
    "    N number of multi-gaussians to fit\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    store_mean : List[float]\n",
    "    store_sigma : List[float]\n",
    "    \"\"\"\n",
    "    store_mean, store_sigma = [], []\n",
    "\n",
    "    # calculate number of rows and columns needed to draw all distributions with upper limit\n",
    "    num = int(len(image_angles)**(1/2)) + 1\n",
    "    print(num)\n",
    "\n",
    "    # if desired, limit number of plots drawn to reduce loading time\n",
    "    #if num > 5:\n",
    "    #    num = 5\n",
    "\n",
    "    fig, ax = plt.subplots(num, num, figsize=(20,20))\n",
    "\n",
    "    # idx used to ID image path as needed\n",
    "    count = 0\n",
    "\n",
    "    for i in range(num):\n",
    "        for j in range(num):\n",
    "    \n",
    "            n, bins, patches = ax[i,j].hist(image_angles[count], 100, density=True, facecolor='green', alpha=0.75)\n",
    "\n",
    "            # if curve_fit params explodes to infinity, use norm.fit for normal distribution approximation\n",
    "            try:\n",
    "                bins_multi = (bins[:-1] + bins[1:]) / 2\n",
    "                fit_params = fit_multi_gaussian(bins_multi, n, n_components=components)\n",
    "    \n",
    "                means = fit_params[1::3].tolist()\n",
    "                stds = fit_params[2::3].tolist()\n",
    "                \n",
    "                store_mean = store_mean + means\n",
    "                store_sigma = store_sigma + stds\n",
    "                \n",
    "                x_min = min(means) - 3 * max(stds)\n",
    "                x_max = max(means) + 3 * max(stds)\n",
    "                x = np.linspace(x_min, x_max, 100)\n",
    "            \n",
    "                for k in range(len(fit_params) // 3):\n",
    "                    amp, mean, std = fit_params[3*k:3*k+3]\n",
    "                    g = amp * np.exp(-((x - mean)**2) / (2 * std**2))\n",
    "                    ax[i,j].plot(x, g, '--', c='red', label=f'Gaussian {k+1}', zorder=4)\n",
    "\n",
    "            except:\n",
    "                (mu, sigma) = norm.fit(np.array(image_angles[count], dtype = float))\n",
    "                #  add a 'best fit' line\n",
    "                y = norm.pdf( bins, mu, sigma)\n",
    "                l = ax[i,j].plot(bins, y, 'r--', linewidth=2)\n",
    "                store_mean = store_mean + [mu]\n",
    "                store_sigma = store_sigma + [sigma]\n",
    "                \n",
    "            ax[i,j].set_xlabel('Angle (radians)',fontsize=20)\n",
    "            ax[i,j].tick_params(\n",
    "                axis='y',          #  changes apply to the y-axis\n",
    "                which='both',      #  both major and minor ticks are affected\n",
    "                left=False,      #  ticks along the bottom edge are off\n",
    "                right=False,         #  ticks along the top edge are off\n",
    "                labelleft=False) #  labels along the bottom edge are off\n",
    "            # ax[i,j].ylabel('Count',fontsize=20)\n",
    "            ax[i,j].text(.01, .99, '%i'%(count), ha='left', va='top', transform=ax[i,j].transAxes,fontsize=20)\n",
    "            ax[i,j].set_xlim(0,np.pi)\n",
    "            count+=1\n",
    "            plt.tight_layout()\n",
    "\n",
    "            if count == len(image_angles):\n",
    "                break\n",
    "        if count == len(image_angles):\n",
    "            break\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    return store_mean, store_sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1bce02-59fa-4ee0-85da-686440112691",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, SIGMA_PRISTINE = plot_hists(ANGLES_PRISTINE_FILTERED[ANGLES_PRISTINE_LABELED['rs_66k']], components=1)\n",
    "\n",
    "_, SIGMA_HD = plot_hists(ANGLES_HD_FILTERED[ANGLES_HD_LABELED['rs_66k']], components=1)\n",
    "\n",
    "_, SIGMA_LD =  plot_hists(ANGLES_LD_FILTERED[ANGLES_LD_LABELED['rs_66k']], components=1)\n",
    "\n",
    "_, SIGMA_88 = plot_hists(ANGLES_88_FILTERED[ANGLES_88_LABELED['rs_66k']], components=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c226175-86c1-4d78-b90a-165d370b94f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_boxplot(data: List[npt.NDArray[float]]) -> None:\n",
    "    labels = ['Pristine n=%i\\n'%len(data[0]) + r'0.0 $\\dfrac{n}{m^{2}}$',\n",
    "              'BR2 Low Dose n=%i\\n'%len(data[1]) + r'$8.23 \\times 10^{21} \\dfrac{n}{m^{2}}$',\n",
    "              'BR2 High Dose n=%i\\n'%len(data[2]) + r'$7.92 \\times 10^{22} \\dfrac{n}{m^{2}}$',\n",
    "              '88-Inch Cyclotron n=%i\\n'%len(data[3]) + r'$1.0 \\times 10^{16} \\dfrac{n}{m^{2}}$',]\n",
    "    \n",
    "    colors = ['#003262', (0,176/255,80/255), 'red', '#FDB515']\n",
    "    mag = ['26k', '38k', '43k', '66k', '92k']\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(23,23))\n",
    "    ax.set_ylabel(r'Angular Dispersion V$\\theta$ (Radians)', fontsize=40)\n",
    "    plt.xticks(fontsize=31)\n",
    "    plt.yticks(fontsize=30)  \n",
    "    \n",
    "    flierprops = dict(marker='o', markerfacecolor='black', markersize=15,\n",
    "                      linestyle='none', markeredgecolor='black')\n",
    "    \n",
    "    \n",
    "    bplot = ax.boxplot(data,\n",
    "                        patch_artist=True,\n",
    "                        flierprops=flierprops,\n",
    "                        showmeans=True,\n",
    "                        meanline=True,\n",
    "                        tick_labels=labels,\n",
    "                        whis=[0, 100])\n",
    "\n",
    "    # fill with colors\n",
    "    for patch, color in zip(bplot['boxes'], colors):\n",
    "        patch.set_facecolor(color)\n",
    "\n",
    "    for median in bplot['medians']:\n",
    "        median.set_color('black')\n",
    "        median.set_linewidth(2)\n",
    "    for mean in bplot['means']:\n",
    "        mean.set_color('black')\n",
    "        mean.set_linewidth(2)\n",
    "        \n",
    "    stats_text = []\n",
    "    for label, category_data in zip(labels, data):\n",
    "        mean_val = np.mean(category_data)\n",
    "        median_val = np.median(category_data)\n",
    "        stats_text.append(f'$\\mu$: {mean_val:.4f}, Median: {median_val:.4f}')\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Add the text at y=1.57\n",
    "    for i in range(1,5):\n",
    "        ax.text(i, -0.035, stats_text[i-1], ha='center', va='center', fontsize=25, color='black')\n",
    "    ax.grid(True, axis='y', linestyle='--', linewidth=2)\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    fig.savefig('/global/cfs/cdirs/m4361/2025/Publication/boxplot_angulardispersion.svg',bbox_inches='tight',pad_inches=0, format='svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6d58b8-1c29-4226-88ad-ed4e262b96f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOXPLOT_DATA = [np.array(SIGMA_PRISTINE),\n",
    "        np.array(SIGMA_LD),\n",
    "        np.array(SIGMA_HD),\n",
    "        np.array(SIGMA_88)]\n",
    "\n",
    "plot_boxplot(BOXPLOT_DATA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7541eea3-6e02-4e70-af35-73176e76e4be",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"N4\"></a>\n",
    "#  References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1543df-56e0-40c7-af15-52ff90bfa593",
   "metadata": {},
   "source": [
    "1 [Suzuki, S. and be, K. (1985).](https://doi.org/10.1016/0734-189X(85)90016-7) Topological structural analysis of digitized binary images by border following. Computer Vision, Graphics, and Image Processing, 30(1), pp.32–46. doi:https://doi.org/10.1016/0734-189x(85)90016-7. <br>\n",
    "2 [Teh, C.-H. . and Chin, R.T. (1989).](https://doi.org/10.1109/34.31447) On the detection of dominant points on digital curves. IEEE Transactions on Pattern Analysis and Machine Intelligence, 11(8), pp.859–872. doi:https://doi.org/10.1109/34.31447. <br>\n",
    "3 [Otsu, N. (1979).](https://doi.org/10.1109/tsmc.1979.4310076) A Threshold Selection Method from Gray-Level Histograms. IEEE Transactions on Systems, Man, and Cybernetics, [online] 9(1), pp.62–66. doi:https://doi.org/10.1109/tsmc.1979.4310076. <br>\n",
    "4 [Zuiderveld, K. (1994).](https://doi.org/10.1016/b978-0-12-336156-1.50061-6) Contrast Limited Adaptive Histogram Equalization. Graphics Gems, pp.474–485. doi:https://doi.org/10.1016/b978-0-12-336156-1.50061-6. <br>\n",
    "5 [Jolliffe, I.T. (2002).](https://doi.org/10.1007/b98835) Principal Component Analysis. [online] Springer Series in Statistics. New York: Springer-Verlag. doi:https://doi.org/10.1007/b98835. <br>\n",
    "6 [Bradski, G. (2000).](https://github.com/opencv/opencv-python) The OpenCV Library. Dr. Dobb's Journal of Software Tools. <br>\n",
    "7 [Virtanen, P. et al. (2020).](https://github.com/scipy/scipy) SciPy 1.0: Fundamental Algorithms for Scientific Computing in \n",
    "Python. Nature Methods, 17, pp.261–272. <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b9db7c",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"N5\"></a>\n",
    "#  Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fa91b8-40ca-4f5f-a712-218a7733ca53",
   "metadata": {},
   "source": [
    "Workflow Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90e8235-673b-4e6d-9f80-f84b0f752d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_PRISTINE = np.array(PATHS_PRISTINE)[ANGLES_PRISTINE_LABELED['rs_66k']][1]\n",
    "IMG_HD = np.array(PATHS_HD)[ANGLES_HD_LABELED['rs_66k']][1]\n",
    "IMG_LD = np.array(PATHS_LD)[ANGLES_LD_LABELED['rs_66k']][8]\n",
    "IMG_88 = np.array(PATHS_88)[ANGLES_88_LABELED['rs_66k']][1]\n",
    "print(IMG_PRISTINE, IMG_HD, IMG_LD, IMG_88)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961b7b83-0035-4493-912c-7eeba86e09bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_idx(img_path, paths: str) -> int:\n",
    "    \n",
    "    i=0\n",
    "    for p in paths:\n",
    "        if p == img_path:\n",
    "            return i\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe14738-9442-47da-b80e-84e1e3a4719c",
   "metadata": {},
   "outputs": [],
   "source": [
    "IDX_P = find_idx(IMG_PRISTINE, PATHS_PRISTINE)\n",
    "IDX_HD = find_idx(IMG_HD, PATHS_HD) + PRISTINE_IDX\n",
    "IDX_LD = find_idx(IMG_LD, PATHS_LD) + HD_IDX\n",
    "IDX_88 = find_idx(IMG_88, PATHS_88) + LD_IDX\n",
    "IDX_P, IDX_HD, IDX_LD, IDX_88"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cf0fda-8102-4360-baf4-b816d78acb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "select_paths = ['/global/cfs/cdirs/m4361/2025/Array/prist/33_66k.tif', \n",
    "                '/global/cfs/cdirs/m4361/2025/Array/HD/296_66k.tif',\n",
    "                '/global/cfs/cdirs/m4361/2025/Array/LD/155_66k.tif',\n",
    "                '/global/cfs/cdirs/m4361/2025/Array/88in/388_66k.tif']\n",
    "\n",
    "for i, path in enumerate(select_paths):\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12,12))\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    fig.subplots_adjust(0,0,1,1)\n",
    "    \n",
    "    ax.imshow(tifffile.imread(path), cmap='gray') # read TIF image in binary\n",
    "\n",
    "    fig.savefig(path.replace(FILE_TYPES[2], '.svg'), bbox_inches='tight', pad_inches = 0, format='svg')\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3260485a-2360-4851-a844-a2b6ae5638cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image(path: str) -> None:\n",
    "    fig, ax = plt.subplots(figsize=(12,12))\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    fig.subplots_adjust(0,0,1,1)\n",
    "    plt.imshow(cv2.imread(path))\n",
    "    \n",
    "    plt.show()\n",
    "    fig.savefig(path.replace('.png', '.svg'), bbox_inches='tight', pad_inches = 0, format='svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6af0d34-6266-49a7-8b13-df8537549a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_image(IMG_PRISTINE.replace('.png', '_pca_final_%i.png'%IDX_P))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2afbc6-62bd-4e7a-87e4-727b2d991eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_image(IMG_HD.replace('.png', '_pca_final_%i.png'%IDX_HD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d728ca19-0ac8-40a2-b6e7-b80b22c41d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_image(IMG_LD.replace('.png', '_pca_final_%i.png'%IDX_LD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb5557f-8b5c-44fe-b8bc-055ffce4b83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_image(IMG_88.replace('.png', '_pca_final_%i.png'%IDX_88))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f280ded7-a266-4afe-b483-073db571ca4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hist(data: npt.NDArray[float], path: str, text: str = '') -> None:\n",
    "    fig,ax=plt.subplots(figsize=(10,10))\n",
    "    \n",
    "    n, bins, patches = ax.hist(data, 100, density=True, facecolor='green', edgecolor='black', alpha=0.75)\n",
    "    \n",
    "    # add a 'best fit' line\n",
    "    bins_multi = (bins[:-1] + bins[1:]) / 2\n",
    "    fit_params = fit_multi_gaussian(bins_multi, n, n_components=1)\n",
    "    \n",
    "    means = fit_params[1::3].tolist()\n",
    "    stds = fit_params[2::3].tolist()\n",
    "    \n",
    "    x_min = min(means) - 3 * max(stds)\n",
    "    x_max = max(means) + 3 * max(stds)\n",
    "    x = np.linspace(x_min, x_max, 100)\n",
    "    \n",
    "    for k in range(len(fit_params) // 3):\n",
    "        amp, mean, std = fit_params[3*k:3*k+3]\n",
    "        g = amp * np.exp(-((x - mean)**2) / (2 * std**2))\n",
    "        ax.plot(x, g, '--', c='red', label=f'Gaussian {k+1}', zorder=4, linewidth=7)\n",
    "        \n",
    "    ax.set_xlabel(r'Angular Dispersion V$\\theta$ (Radians)', fontsize=30)\n",
    "    ax.text(.01, .99, text + r': $\\mu$=%.4f, $\\sigma$=%.4f'%(means[0], stds[0]), ha='left', va='top', transform=ax.transAxes,fontsize=28)\n",
    "    plt.xticks(fontsize=25)\n",
    "    ax.tick_params(axis='y', which='both', length=0)\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_xlim(0,np.pi)\n",
    "    plt.tight_layout()\n",
    "    ax.grid(True, axis='x', linestyle='--', linewidth=2)\n",
    "    plt.show()\n",
    "    fig.savefig(path.replace('.tif', '_anghist.svg'), bbox_inches='tight', pad_inches = 0, format='svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d818a99e-66fe-449e-ac64-ed47efc8b516",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hist(ANGLES_PRISTINE_FILTERED[ANGLES_PRISTINE_LABELED['rs_66k']][1], path = select_paths[0], text='Pristine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb49464-9c9b-4f0b-b6c6-8d621aa200a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hist(ANGLES_HD_FILTERED[ANGLES_HD_LABELED['rs_66k']][1], path = select_paths[1], text='HD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2705d1ac-3ae3-4620-9485-035dad3e6bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hist(ANGLES_LD_FILTERED[ANGLES_LD_LABELED['rs_66k']][8], path = select_paths[2], text='LD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1fc7fe-eca1-404a-a72e-273a84f34e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hist(ANGLES_88_FILTERED[ANGLES_88_LABELED['rs_66k']][1], path = select_paths[3], text='88 Inch')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31223f1-45d4-4862-80d3-641e4b6ee4da",
   "metadata": {},
   "source": [
    "Read other types of file types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b73d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dm3(path: str) -> tuple[npt.NDArray[float], float, float]:\n",
    "    \"\"\"\n",
    "    Retrieve the \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    image_directory : str\n",
    "    Path to directory with image files\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    paths : List[str]\n",
    "    List of paths to images found in image directory\n",
    "    \"\"\"\n",
    "    dmfile = ncempy.io.dm.fileDM(path)\n",
    "    tags = dmfile.allTags\n",
    "\n",
    "    lower_bound = tags['.DocumentObjectList.1.ImageDisplayInfo.LowLimit']\n",
    "    upper_bound = tags['.DocumentObjectList.1.ImageDisplayInfo.HighLimit']\n",
    "    ax1_scale = tags['.ImageList.2.ImageData.Calibrations.Dimension.1.Scale']\n",
    "    ax1_units = tags['.ImageList.2.ImageData.Calibrations.Dimension.1.Units']\n",
    "    ax2_scale = tags['.ImageList.2.ImageData.Calibrations.Dimension.2.Scale']\n",
    "    ax2_units = tags['.ImageList.2.ImageData.Calibrations.Dimension.2.Units']\n",
    "\n",
    "    return dmfile.getDataset(0)['data'], lower_bound, upper_bound\n",
    "\n",
    "def threshold_dm3(data: np.ndarray, lower_bound: float, upper_bound:float, scale: bool = False) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Retrieve the \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    image_directory : str\n",
    "    Path to directory with image files\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    paths : List[str]\n",
    "    List of paths to images found in image directory\n",
    "    \"\"\"\n",
    "    data = data.clip(lower_bound, upper_bound)\n",
    "    if scale:\n",
    "        data = (data - lower_bound) / upper_bound * 255\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4a614b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dm3_to_image(paths: str) -> tuple[List[float], List[str], List[float], List[str]]:\n",
    "    \"\"\"\n",
    "    Retrieve the \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    image_directory : str\n",
    "    Path to directory with image files\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    paths : List[str]\n",
    "    List of paths to images found in image directory\n",
    "    \"\"\"\n",
    "    datatensor1024 = torch.tensor([])\n",
    "    datatensor512 = torch.tensor([])\n",
    "    path1024 = []\n",
    "    path512 = []\n",
    "    count = 0\n",
    "    \n",
    "    for i, path in enumerate(paths):\n",
    "        try:\n",
    "            data, lower_bound, upper_bound = read_dm3(path)\n",
    "            data = threshold_dm3(data, lower_bound, upper_bound, scale=True)\n",
    "        except:\n",
    "            count += 1\n",
    "            continue\n",
    "        if data.shape[0] == 1024 and data.shape[1] == 1024:\n",
    "            datatensor1024 = torch.cat((datatensor1024, torch.tensor(data)[:,:,None]), -1)\n",
    "            path1024.append(path)\n",
    "        if data.shape[0] == 512 and data.shape[1] == 512:\n",
    "            datatensor512 = torch.cat((datatensor512, torch.tensor(data)[:,:,None]), -1)\n",
    "            path512.append(path)\n",
    "    print(count)\n",
    "\n",
    "    return datatensor1024, path1024, datatensor512, path512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d64b1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PRISTINE1024, PATH_PRISTINE1024, DATA_PRISTINE512, PATH_PRISTINE512 = dm3_to_image(PATHS_PRISTINE)\n",
    "DATA_HD1024, PATH_HD1024, DATA_HD512, PATH_HD512 = dm3_to_image(PATHS_HD)\n",
    "DATA_LD1024, PATH_LD1024, DATA_LD512, PATH_LD512 = dm3_to_image(PATHS_LD)\n",
    "DATA_881024, PATH_881024, DATA_88512, PATH_88512 = dm3_to_image(PATHS_88)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e586ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dm5_to_image(paths: str) -> tuple[List[float], List[str]]:\n",
    "    \"\"\"\n",
    "    Retrieve the \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    image_directory : str\n",
    "    Path to directory with image files\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    paths : List[str]\n",
    "    List of paths to images found in image directory\n",
    "    \"\"\"\n",
    "    datatensor = torch.tensor([])\n",
    "    path = []\n",
    "\n",
    "    error_count = 0\n",
    "    \n",
    "    for i, path in enumerate(paths):\n",
    "        try:\n",
    "            with h5py.File(path, 'r') as f:\n",
    "                data = f['ImageList/[0]/ImageData/Data'][()][:,:,0]\n",
    "        except:\n",
    "            count += 1\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            datatensor = torch.cat((datatensor, torch.tensor(data)[:,:,None]), -1)\n",
    "            path.append(path)\n",
    "        except:\n",
    "            error_count += 1\n",
    "       \n",
    "    print(error_count)\n",
    "\n",
    "    return datatensor, path"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Proj46",
   "language": "python",
   "name": "proj46"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
